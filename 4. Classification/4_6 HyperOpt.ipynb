{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f47a0f3",
   "metadata": {},
   "source": [
    "### 베이지안 최적화 개요와 HyperOpt 사용법\n",
    "- 목적함수, 검색공간 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9955eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.7\n"
     ]
    }
   ],
   "source": [
    "import hyperopt\n",
    "\n",
    "print(hyperopt.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0ae529",
   "metadata": {},
   "source": [
    "- 검색공간 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8480077e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': <hyperopt.pyll.base.Apply at 0x23da1c67290>,\n",
       " 'y': <hyperopt.pyll.base.Apply at 0x23da1ca4710>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hyperopt import hp \n",
    "\n",
    "# 검색공간 (최적화 파라미터를 입력 시킬 값)\n",
    "# -10 ~ 10까지 1간격을 가지는 입력 변수 x 집합값과 -15 ~ 15까지 1간격을 가지는 입력 변수  y 집합값 설정.\n",
    "search_space = {'x' : hp.quniform('x', -10, 10, 1), 'y' : hp.quniform('y', -15, 15, 1)}\n",
    "search_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ef613",
   "metadata": {},
   "source": [
    "- 목적함수 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d89f38ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK\n",
    "\n",
    "# 목적함수 생성 \n",
    "# 입력 변수값과 입력 변수 검색 범위를 가지는 딕셔너리를 인자로 받고,  특정 값을 반환 \n",
    "def objective_func(search_space):\n",
    "    x = search_space['x']\n",
    "    y = search_space['y']\n",
    "    retval = x**2 - 20*y\n",
    "\n",
    "    return retval # return {'loss': retval, 'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5be0b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 714.95trial/s, best loss: -224.0]\n",
      "best : {'x': np.float64(-4.0), 'y': np.float64(12.0)}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, Trials\n",
    "import numpy as np \n",
    "\n",
    "# 입력 결괏값을 저장한 Trials 객체값 생성 \n",
    "trial_val = Trials()\n",
    "\n",
    "# 목적 함수의 최솟값을 반환하는 최적 입력 변숫값을 5번의 입력값 시도(max_evals=5)로 찾아냄.\n",
    "best_01 = fmin(fn=objective_func, space=search_space, algo=tpe.suggest, max_evals=5,\n",
    "               trials=trial_val, rstate=np.random.default_rng(seed=0))\n",
    "\n",
    "print('best :', best_01)\n",
    "# rstate : 일반적인 정수형 값 x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acb89e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 1052.68trial/s, best loss: -296.0]\n",
      "best: {'x': np.float64(2.0), 'y': np.float64(15.0)}\n"
     ]
    }
   ],
   "source": [
    "trial_val = Trials()\n",
    "\n",
    "# max_evals를 20회로 늘려서 재테스트\n",
    "best_02 = fmin(fn=objective_func, space=search_space, algo=tpe.suggest, max_evals=20\n",
    "               , trials=trial_val, rstate=np.random.default_rng(seed=0))\n",
    "print('best:', best_02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb597f0",
   "metadata": {},
   "source": [
    "- HyperOpt 수행시 적용된 입력값들과 목적함수 반환값 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1bedf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'loss': -64.0, 'status': 'ok'}, {'loss': -184.0, 'status': 'ok'}, {'loss': 56.0, 'status': 'ok'}, {'loss': -224.0, 'status': 'ok'}, {'loss': 61.0, 'status': 'ok'}, {'loss': -296.0, 'status': 'ok'}, {'loss': -40.0, 'status': 'ok'}, {'loss': 281.0, 'status': 'ok'}, {'loss': 64.0, 'status': 'ok'}, {'loss': 100.0, 'status': 'ok'}, {'loss': 60.0, 'status': 'ok'}, {'loss': -39.0, 'status': 'ok'}, {'loss': 1.0, 'status': 'ok'}, {'loss': -164.0, 'status': 'ok'}, {'loss': 21.0, 'status': 'ok'}, {'loss': -56.0, 'status': 'ok'}, {'loss': 284.0, 'status': 'ok'}, {'loss': 176.0, 'status': 'ok'}, {'loss': -171.0, 'status': 'ok'}, {'loss': 0.0, 'status': 'ok'}]\n"
     ]
    }
   ],
   "source": [
    "# results속성 : 목적함수 반환값들이 저장 {'loss':함수 반환값, 'status':반환 상태값}\n",
    "print(trial_val.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "483e8470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': [np.float64(-6.0), np.float64(-4.0), np.float64(4.0), np.float64(-4.0), np.float64(9.0), np.float64(2.0), np.float64(10.0), np.float64(-9.0), np.float64(-8.0), np.float64(-0.0), np.float64(-0.0), np.float64(1.0), np.float64(9.0), np.float64(6.0), np.float64(9.0), np.float64(2.0), np.float64(-2.0), np.float64(-4.0), np.float64(7.0), np.float64(-0.0)], 'y': [np.float64(5.0), np.float64(10.0), np.float64(-2.0), np.float64(12.0), np.float64(1.0), np.float64(15.0), np.float64(7.0), np.float64(-10.0), np.float64(0.0), np.float64(-5.0), np.float64(-3.0), np.float64(2.0), np.float64(4.0), np.float64(10.0), np.float64(3.0), np.float64(3.0), np.float64(-14.0), np.float64(-8.0), np.float64(11.0), np.float64(-0.0)]}\n"
     ]
    }
   ],
   "source": [
    "# vals 속성 : 입력 변수값들 반환 {'입력변수명':개별 수행 시마다 입력된 값 리스트}\n",
    "print(trial_val.vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95dab110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>losses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-9.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>281.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-2.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>284.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-171.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x     y  losses\n",
       "0   -6.0   5.0   -64.0\n",
       "1   -4.0  10.0  -184.0\n",
       "2    4.0  -2.0    56.0\n",
       "3   -4.0  12.0  -224.0\n",
       "4    9.0   1.0    61.0\n",
       "5    2.0  15.0  -296.0\n",
       "6   10.0   7.0   -40.0\n",
       "7   -9.0 -10.0   281.0\n",
       "8   -8.0   0.0    64.0\n",
       "9   -0.0  -5.0   100.0\n",
       "10  -0.0  -3.0    60.0\n",
       "11   1.0   2.0   -39.0\n",
       "12   9.0   4.0     1.0\n",
       "13   6.0  10.0  -164.0\n",
       "14   9.0   3.0    21.0\n",
       "15   2.0   3.0   -56.0\n",
       "16  -2.0 -14.0   284.0\n",
       "17  -4.0  -8.0   176.0\n",
       "18   7.0  11.0  -171.0\n",
       "19  -0.0  -0.0     0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# results에서 loss 키값에 해당하는 value들을 추출하여 list로 생성 \n",
    "losses = [loss_dict['loss'] for loss_dict in trial_val.results]\n",
    "\n",
    "# DataFrame으로 생성. \n",
    "result_df = pd.DataFrame({'x': trial_val.vals['x'],\n",
    "                         'y': trial_val.vals['y'],\n",
    "                          'losses': losses\n",
    "                         }\n",
    "                        )\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fc1933",
   "metadata": {},
   "source": [
    "### HyperOpt를 XGBoost 하이퍼 파라미터 튜닝에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ace7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "cancer_df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "cancer_df['target'] = dataset.target\n",
    "\n",
    "X_features = cancer_df.iloc[:, :-1]\n",
    "y_label = cancer_df.iloc[:, -1]\n",
    "\n",
    "# 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출\n",
    "X_train, X_test, y_train, y_test=train_test_split(X_features, y_label,\n",
    "                                         test_size=0.2, random_state=156 )\n",
    "\n",
    "# 학습 데이터를 다시 학습과 검증 데이터로 분리 \n",
    "X_tr, X_val, y_tr, y_val= train_test_split(X_train, y_train,\n",
    "                                         test_size=0.1, random_state=156 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8198694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "\n",
    "# max_depth는 5에서 20까지 1간격으로, min_child_weight는 1에서 2까지 1간격으로\n",
    "# colsample_bytree는 0.5에서 1사이, learning_rate는 0.01에서 0.2사이 정규 분포된 값으로 검색. \n",
    "xgb_search_space = {'max_depth': hp.quniform('max_depth', 5, 20, 1),\n",
    "                    'min_child_weight': hp.quniform('min_child_weight', 1, 2, 1),\n",
    "                    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "                    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1)\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1abf7bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from hyperopt import STATUS_OK\n",
    "\n",
    "# fmin()에 입력된 검색공간은 모두 실수형 \n",
    "# XGBClassifier의 정수형 하이퍼 파라미터는 정수형으로 변환 필요 \n",
    "# 정확도는 높을 수록 더 좋은 수치 \n",
    "# -1 * 정확도를 곱해서 큰 정확도 값일 수록 최소가 되도록 변환 0.8 < 0.9 (fmin()은 최솟값을 최적화함) -> -0.8 > -0.9 \n",
    "def objective_func(search_space):\n",
    "    xgb_clf = XGBClassifier(n_estimators=100, max_depth=int(search_space['max_depth']),\n",
    "                            min_child_weight=int(search_space['min_child_weight']),\n",
    "                            learning_rate=search_space['learning_rate'],\n",
    "                            colsample_bytree=search_space['colsample_bytree'], \n",
    "                            eval_metric='logloss')\n",
    "    \n",
    "    accuracy = cross_val_score(xgb_clf, X_train, y_train, scoring='accuracy', cv=3)\n",
    "\n",
    "    # accuracy는 cv=3 개수만큼의 정확도 결과를 가지므로 이를 평균해서 반환하되 -1을 곱해줌. \n",
    "    return {'loss':-1 * np.mean(accuracy), 'status': STATUS_OK}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6df3895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:12<00:00,  4.09trial/s, best loss: -0.967047170907401] \n",
      "best: {'colsample_bytree': np.float64(0.7923560109541058), 'learning_rate': np.float64(0.1938940946651398), 'max_depth': np.float64(16.0), 'min_child_weight': np.float64(2.0)}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, Trials\n",
    "\n",
    "trial_val = Trials()\n",
    "best = fmin(fn=objective_func,\n",
    "            space=xgb_search_space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50, # 최대 반복 횟수를 지정합니다.\n",
    "            trials=trial_val, rstate=np.random.default_rng(seed=9))\n",
    "print('best:', best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12357069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colsample_bytree:0.79236, learning_rate:0.19389, max_depth:16, min_child_weight:2\n"
     ]
    }
   ],
   "source": [
    "print('colsample_bytree:{0}, learning_rate:{1}, max_depth:{2}, min_child_weight:{3}'.format(\n",
    "                        round(best['colsample_bytree'], 5), round(best['learning_rate'], 5),\n",
    "                        int(best['max_depth']), int(best['min_child_weight'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adb44f0",
   "metadata": {},
   "source": [
    "- 위의 최적화 하이퍼 파라미터로 재학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52b7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix( y_test, pred)\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    precision = precision_score(y_test , pred)\n",
    "    recall = recall_score(y_test , pred)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    # ROC-AUC 추가 \n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    print('오차 행렬')\n",
    "    print(confusion)\n",
    "    # ROC-AUC print 추가\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
    "    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "100e03d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.52017\tvalidation_1-logloss:0.56664\n",
      "[1]\tvalidation_0-logloss:0.41762\tvalidation_1-logloss:0.49501\n",
      "[2]\tvalidation_0-logloss:0.34123\tvalidation_1-logloss:0.43295\n",
      "[3]\tvalidation_0-logloss:0.28440\tvalidation_1-logloss:0.39091\n",
      "[4]\tvalidation_0-logloss:0.24056\tvalidation_1-logloss:0.35565\n",
      "[5]\tvalidation_0-logloss:0.20676\tvalidation_1-logloss:0.33910\n",
      "[6]\tvalidation_0-logloss:0.17704\tvalidation_1-logloss:0.31555\n",
      "[7]\tvalidation_0-logloss:0.15563\tvalidation_1-logloss:0.29982\n",
      "[8]\tvalidation_0-logloss:0.13628\tvalidation_1-logloss:0.28915\n",
      "[9]\tvalidation_0-logloss:0.11984\tvalidation_1-logloss:0.27626\n",
      "[10]\tvalidation_0-logloss:0.10728\tvalidation_1-logloss:0.27569\n",
      "[11]\tvalidation_0-logloss:0.09639\tvalidation_1-logloss:0.27079\n",
      "[12]\tvalidation_0-logloss:0.08691\tvalidation_1-logloss:0.26944\n",
      "[13]\tvalidation_0-logloss:0.07998\tvalidation_1-logloss:0.26511\n",
      "[14]\tvalidation_0-logloss:0.07359\tvalidation_1-logloss:0.26765\n",
      "[15]\tvalidation_0-logloss:0.06817\tvalidation_1-logloss:0.26790\n",
      "[16]\tvalidation_0-logloss:0.06358\tvalidation_1-logloss:0.26820\n",
      "[17]\tvalidation_0-logloss:0.05831\tvalidation_1-logloss:0.26605\n",
      "[18]\tvalidation_0-logloss:0.05449\tvalidation_1-logloss:0.26464\n",
      "[19]\tvalidation_0-logloss:0.05087\tvalidation_1-logloss:0.26190\n",
      "[20]\tvalidation_0-logloss:0.04734\tvalidation_1-logloss:0.26330\n",
      "[21]\tvalidation_0-logloss:0.04406\tvalidation_1-logloss:0.25782\n",
      "[22]\tvalidation_0-logloss:0.04097\tvalidation_1-logloss:0.25848\n",
      "[23]\tvalidation_0-logloss:0.03889\tvalidation_1-logloss:0.25730\n",
      "[24]\tvalidation_0-logloss:0.03674\tvalidation_1-logloss:0.25660\n",
      "[25]\tvalidation_0-logloss:0.03510\tvalidation_1-logloss:0.25804\n",
      "[26]\tvalidation_0-logloss:0.03349\tvalidation_1-logloss:0.25645\n",
      "[27]\tvalidation_0-logloss:0.03152\tvalidation_1-logloss:0.25428\n",
      "[28]\tvalidation_0-logloss:0.03017\tvalidation_1-logloss:0.25443\n",
      "[29]\tvalidation_0-logloss:0.02943\tvalidation_1-logloss:0.25697\n",
      "[30]\tvalidation_0-logloss:0.02853\tvalidation_1-logloss:0.25700\n",
      "[31]\tvalidation_0-logloss:0.02786\tvalidation_1-logloss:0.25552\n",
      "[32]\tvalidation_0-logloss:0.02703\tvalidation_1-logloss:0.25547\n",
      "[33]\tvalidation_0-logloss:0.02640\tvalidation_1-logloss:0.25504\n",
      "[34]\tvalidation_0-logloss:0.02568\tvalidation_1-logloss:0.25555\n",
      "[35]\tvalidation_0-logloss:0.02539\tvalidation_1-logloss:0.25353\n",
      "[36]\tvalidation_0-logloss:0.02453\tvalidation_1-logloss:0.24781\n",
      "[37]\tvalidation_0-logloss:0.02391\tvalidation_1-logloss:0.24902\n",
      "[38]\tvalidation_0-logloss:0.02338\tvalidation_1-logloss:0.24682\n",
      "[39]\tvalidation_0-logloss:0.02315\tvalidation_1-logloss:0.24558\n",
      "[40]\tvalidation_0-logloss:0.02294\tvalidation_1-logloss:0.24809\n",
      "[41]\tvalidation_0-logloss:0.02239\tvalidation_1-logloss:0.24934\n",
      "[42]\tvalidation_0-logloss:0.02218\tvalidation_1-logloss:0.24907\n",
      "[43]\tvalidation_0-logloss:0.02199\tvalidation_1-logloss:0.24890\n",
      "[44]\tvalidation_0-logloss:0.02144\tvalidation_1-logloss:0.24449\n",
      "[45]\tvalidation_0-logloss:0.02122\tvalidation_1-logloss:0.24716\n",
      "[46]\tvalidation_0-logloss:0.02103\tvalidation_1-logloss:0.24550\n",
      "[47]\tvalidation_0-logloss:0.02085\tvalidation_1-logloss:0.24799\n",
      "[48]\tvalidation_0-logloss:0.02069\tvalidation_1-logloss:0.24506\n",
      "[49]\tvalidation_0-logloss:0.02054\tvalidation_1-logloss:0.24488\n",
      "[50]\tvalidation_0-logloss:0.02039\tvalidation_1-logloss:0.24343\n",
      "[51]\tvalidation_0-logloss:0.02022\tvalidation_1-logloss:0.24591\n",
      "[52]\tvalidation_0-logloss:0.02008\tvalidation_1-logloss:0.24272\n",
      "[53]\tvalidation_0-logloss:0.01993\tvalidation_1-logloss:0.24322\n",
      "[54]\tvalidation_0-logloss:0.01980\tvalidation_1-logloss:0.24546\n",
      "[55]\tvalidation_0-logloss:0.01965\tvalidation_1-logloss:0.24408\n",
      "[56]\tvalidation_0-logloss:0.01951\tvalidation_1-logloss:0.24389\n",
      "[57]\tvalidation_0-logloss:0.01938\tvalidation_1-logloss:0.24127\n",
      "[58]\tvalidation_0-logloss:0.01926\tvalidation_1-logloss:0.24349\n",
      "[59]\tvalidation_0-logloss:0.01913\tvalidation_1-logloss:0.24221\n",
      "[60]\tvalidation_0-logloss:0.01901\tvalidation_1-logloss:0.24265\n",
      "[61]\tvalidation_0-logloss:0.01889\tvalidation_1-logloss:0.24251\n",
      "[62]\tvalidation_0-logloss:0.01878\tvalidation_1-logloss:0.23958\n",
      "[63]\tvalidation_0-logloss:0.01867\tvalidation_1-logloss:0.23807\n",
      "[64]\tvalidation_0-logloss:0.01855\tvalidation_1-logloss:0.23788\n",
      "[65]\tvalidation_0-logloss:0.01843\tvalidation_1-logloss:0.24010\n",
      "[66]\tvalidation_0-logloss:0.01832\tvalidation_1-logloss:0.24056\n",
      "[67]\tvalidation_0-logloss:0.01822\tvalidation_1-logloss:0.23819\n",
      "[68]\tvalidation_0-logloss:0.01812\tvalidation_1-logloss:0.23702\n",
      "[69]\tvalidation_0-logloss:0.01802\tvalidation_1-logloss:0.23818\n",
      "[70]\tvalidation_0-logloss:0.01791\tvalidation_1-logloss:0.24028\n",
      "[71]\tvalidation_0-logloss:0.01780\tvalidation_1-logloss:0.23737\n",
      "[72]\tvalidation_0-logloss:0.01770\tvalidation_1-logloss:0.23719\n",
      "[73]\tvalidation_0-logloss:0.01761\tvalidation_1-logloss:0.23579\n",
      "[74]\tvalidation_0-logloss:0.01752\tvalidation_1-logloss:0.23566\n",
      "[75]\tvalidation_0-logloss:0.01744\tvalidation_1-logloss:0.23676\n",
      "[76]\tvalidation_0-logloss:0.01734\tvalidation_1-logloss:0.23408\n",
      "[77]\tvalidation_0-logloss:0.01726\tvalidation_1-logloss:0.23516\n",
      "[78]\tvalidation_0-logloss:0.01718\tvalidation_1-logloss:0.23268\n",
      "[79]\tvalidation_0-logloss:0.01710\tvalidation_1-logloss:0.23051\n",
      "[80]\tvalidation_0-logloss:0.01701\tvalidation_1-logloss:0.23280\n",
      "[81]\tvalidation_0-logloss:0.01693\tvalidation_1-logloss:0.23259\n",
      "[82]\tvalidation_0-logloss:0.01685\tvalidation_1-logloss:0.23263\n",
      "[83]\tvalidation_0-logloss:0.01677\tvalidation_1-logloss:0.23160\n",
      "[84]\tvalidation_0-logloss:0.01670\tvalidation_1-logloss:0.23236\n",
      "[85]\tvalidation_0-logloss:0.01662\tvalidation_1-logloss:0.23007\n",
      "[86]\tvalidation_0-logloss:0.01655\tvalidation_1-logloss:0.23018\n",
      "[87]\tvalidation_0-logloss:0.01648\tvalidation_1-logloss:0.22995\n",
      "[88]\tvalidation_0-logloss:0.01641\tvalidation_1-logloss:0.22985\n",
      "[89]\tvalidation_0-logloss:0.01634\tvalidation_1-logloss:0.23198\n",
      "[90]\tvalidation_0-logloss:0.01627\tvalidation_1-logloss:0.23185\n",
      "[91]\tvalidation_0-logloss:0.01621\tvalidation_1-logloss:0.23259\n",
      "[92]\tvalidation_0-logloss:0.01613\tvalidation_1-logloss:0.23162\n",
      "[93]\tvalidation_0-logloss:0.01606\tvalidation_1-logloss:0.22943\n",
      "[94]\tvalidation_0-logloss:0.01601\tvalidation_1-logloss:0.22964\n",
      "[95]\tvalidation_0-logloss:0.01594\tvalidation_1-logloss:0.23167\n",
      "[96]\tvalidation_0-logloss:0.01588\tvalidation_1-logloss:0.23178\n",
      "[97]\tvalidation_0-logloss:0.01581\tvalidation_1-logloss:0.22970\n",
      "[98]\tvalidation_0-logloss:0.01575\tvalidation_1-logloss:0.22958\n",
      "[99]\tvalidation_0-logloss:0.01569\tvalidation_1-logloss:0.22971\n",
      "[100]\tvalidation_0-logloss:0.01563\tvalidation_1-logloss:0.22775\n",
      "[101]\tvalidation_0-logloss:0.01557\tvalidation_1-logloss:0.22855\n",
      "[102]\tvalidation_0-logloss:0.01551\tvalidation_1-logloss:0.22790\n",
      "[103]\tvalidation_0-logloss:0.01545\tvalidation_1-logloss:0.22711\n",
      "[104]\tvalidation_0-logloss:0.01540\tvalidation_1-logloss:0.22909\n",
      "[105]\tvalidation_0-logloss:0.01534\tvalidation_1-logloss:0.22881\n",
      "[106]\tvalidation_0-logloss:0.01528\tvalidation_1-logloss:0.22898\n",
      "[107]\tvalidation_0-logloss:0.01523\tvalidation_1-logloss:0.22715\n",
      "[108]\tvalidation_0-logloss:0.01517\tvalidation_1-logloss:0.22794\n",
      "[109]\tvalidation_0-logloss:0.01512\tvalidation_1-logloss:0.22996\n",
      "[110]\tvalidation_0-logloss:0.01508\tvalidation_1-logloss:0.22971\n",
      "[111]\tvalidation_0-logloss:0.01502\tvalidation_1-logloss:0.22891\n",
      "[112]\tvalidation_0-logloss:0.01497\tvalidation_1-logloss:0.23075\n",
      "[113]\tvalidation_0-logloss:0.01492\tvalidation_1-logloss:0.23095\n",
      "[114]\tvalidation_0-logloss:0.01487\tvalidation_1-logloss:0.22907\n",
      "[115]\tvalidation_0-logloss:0.01482\tvalidation_1-logloss:0.22922\n",
      "[116]\tvalidation_0-logloss:0.01478\tvalidation_1-logloss:0.22856\n",
      "[117]\tvalidation_0-logloss:0.01473\tvalidation_1-logloss:0.22878\n",
      "[118]\tvalidation_0-logloss:0.01469\tvalidation_1-logloss:0.22851\n",
      "[119]\tvalidation_0-logloss:0.01464\tvalidation_1-logloss:0.22929\n",
      "[120]\tvalidation_0-logloss:0.01460\tvalidation_1-logloss:0.22859\n",
      "[121]\tvalidation_0-logloss:0.01455\tvalidation_1-logloss:0.22798\n",
      "[122]\tvalidation_0-logloss:0.01451\tvalidation_1-logloss:0.22821\n",
      "[123]\tvalidation_0-logloss:0.01447\tvalidation_1-logloss:0.22660\n",
      "[124]\tvalidation_0-logloss:0.01442\tvalidation_1-logloss:0.22738\n",
      "[125]\tvalidation_0-logloss:0.01438\tvalidation_1-logloss:0.22918\n",
      "[126]\tvalidation_0-logloss:0.01434\tvalidation_1-logloss:0.22855\n",
      "[127]\tvalidation_0-logloss:0.01430\tvalidation_1-logloss:0.22827\n",
      "[128]\tvalidation_0-logloss:0.01426\tvalidation_1-logloss:0.23006\n",
      "[129]\tvalidation_0-logloss:0.01422\tvalidation_1-logloss:0.22852\n",
      "[130]\tvalidation_0-logloss:0.01418\tvalidation_1-logloss:0.22879\n",
      "[131]\tvalidation_0-logloss:0.01414\tvalidation_1-logloss:0.22953\n",
      "[132]\tvalidation_0-logloss:0.01410\tvalidation_1-logloss:0.22896\n",
      "[133]\tvalidation_0-logloss:0.01407\tvalidation_1-logloss:0.22839\n",
      "[134]\tvalidation_0-logloss:0.01403\tvalidation_1-logloss:0.22812\n",
      "[135]\tvalidation_0-logloss:0.01399\tvalidation_1-logloss:0.22978\n",
      "[136]\tvalidation_0-logloss:0.01396\tvalidation_1-logloss:0.23006\n",
      "[137]\tvalidation_0-logloss:0.01393\tvalidation_1-logloss:0.22983\n",
      "[138]\tvalidation_0-logloss:0.01389\tvalidation_1-logloss:0.22928\n",
      "[139]\tvalidation_0-logloss:0.01386\tvalidation_1-logloss:0.22940\n",
      "[140]\tvalidation_0-logloss:0.01383\tvalidation_1-logloss:0.22967\n",
      "[141]\tvalidation_0-logloss:0.01380\tvalidation_1-logloss:0.23125\n",
      "[142]\tvalidation_0-logloss:0.01377\tvalidation_1-logloss:0.22984\n",
      "[143]\tvalidation_0-logloss:0.01374\tvalidation_1-logloss:0.23058\n",
      "[144]\tvalidation_0-logloss:0.01371\tvalidation_1-logloss:0.23008\n",
      "[145]\tvalidation_0-logloss:0.01368\tvalidation_1-logloss:0.22985\n",
      "[146]\tvalidation_0-logloss:0.01365\tvalidation_1-logloss:0.22936\n",
      "[147]\tvalidation_0-logloss:0.01362\tvalidation_1-logloss:0.23083\n",
      "[148]\tvalidation_0-logloss:0.01359\tvalidation_1-logloss:0.23113\n",
      "[149]\tvalidation_0-logloss:0.01356\tvalidation_1-logloss:0.23092\n",
      "[150]\tvalidation_0-logloss:0.01353\tvalidation_1-logloss:0.23046\n",
      "[151]\tvalidation_0-logloss:0.01350\tvalidation_1-logloss:0.23120\n",
      "[152]\tvalidation_0-logloss:0.01348\tvalidation_1-logloss:0.23093\n",
      "[153]\tvalidation_0-logloss:0.01345\tvalidation_1-logloss:0.23121\n",
      "[154]\tvalidation_0-logloss:0.01342\tvalidation_1-logloss:0.23074\n",
      "[155]\tvalidation_0-logloss:0.01340\tvalidation_1-logloss:0.23212\n",
      "[156]\tvalidation_0-logloss:0.01337\tvalidation_1-logloss:0.23190\n",
      "[157]\tvalidation_0-logloss:0.01335\tvalidation_1-logloss:0.23146\n",
      "[158]\tvalidation_0-logloss:0.01332\tvalidation_1-logloss:0.23215\n",
      "[159]\tvalidation_0-logloss:0.01330\tvalidation_1-logloss:0.23188\n",
      "[160]\tvalidation_0-logloss:0.01327\tvalidation_1-logloss:0.23217\n",
      "[161]\tvalidation_0-logloss:0.01325\tvalidation_1-logloss:0.23197\n",
      "[162]\tvalidation_0-logloss:0.01323\tvalidation_1-logloss:0.23154\n",
      "[163]\tvalidation_0-logloss:0.01320\tvalidation_1-logloss:0.23113\n",
      "[164]\tvalidation_0-logloss:0.01318\tvalidation_1-logloss:0.23121\n",
      "[165]\tvalidation_0-logloss:0.01316\tvalidation_1-logloss:0.23257\n",
      "[166]\tvalidation_0-logloss:0.01314\tvalidation_1-logloss:0.23238\n",
      "[167]\tvalidation_0-logloss:0.01312\tvalidation_1-logloss:0.23197\n",
      "[168]\tvalidation_0-logloss:0.01309\tvalidation_1-logloss:0.23264\n",
      "[169]\tvalidation_0-logloss:0.01307\tvalidation_1-logloss:0.23224\n",
      "[170]\tvalidation_0-logloss:0.01305\tvalidation_1-logloss:0.23253\n",
      "[171]\tvalidation_0-logloss:0.01303\tvalidation_1-logloss:0.23234\n",
      "[172]\tvalidation_0-logloss:0.01301\tvalidation_1-logloss:0.23191\n",
      "[173]\tvalidation_0-logloss:0.01299\tvalidation_1-logloss:0.23255\n",
      "오차 행렬\n",
      "[[34  3]\n",
      " [ 4 73]]\n",
      "정확도: 0.9386, 정밀도: 0.9605, 재현율: 0.9481,    F1: 0.9542, AUC:0.9940\n"
     ]
    }
   ],
   "source": [
    "xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=round(best['learning_rate'], 5), \n",
    "                            max_depth=int(best['max_depth']), min_child_weight=int(best['min_child_weight']),\n",
    "                            colsample_bytree=round(best['colsample_bytree'], 5), early_stopping_rounds=50, eval_metric='logloss'\n",
    "                           )\n",
    "\n",
    "evals = [(X_tr, y_tr), (X_val, y_val)]\n",
    "xgb_wrapper.fit(X_tr, y_tr, \n",
    "                eval_set=evals, verbose=True)\n",
    "\n",
    "preds = xgb_wrapper.predict(X_test)\n",
    "pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]\n",
    "\n",
    "get_clf_eval(y_test, preds, pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef5fbc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>losses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.585235</td>\n",
       "      <td>0.033688</td>\n",
       "      <td>-0.945117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.727186</td>\n",
       "      <td>0.105956</td>\n",
       "      <td>-0.962676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.959945</td>\n",
       "      <td>0.154804</td>\n",
       "      <td>-0.960454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.120686</td>\n",
       "      <td>-0.960483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.674336</td>\n",
       "      <td>0.142392</td>\n",
       "      <td>-0.960454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.863774</td>\n",
       "      <td>0.106579</td>\n",
       "      <td>-0.960454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.957521</td>\n",
       "      <td>0.079111</td>\n",
       "      <td>-0.956082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.695018</td>\n",
       "      <td>0.095213</td>\n",
       "      <td>-0.960483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.684442</td>\n",
       "      <td>0.147520</td>\n",
       "      <td>-0.960483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.592116</td>\n",
       "      <td>0.081179</td>\n",
       "      <td>-0.953904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.614798</td>\n",
       "      <td>0.076255</td>\n",
       "      <td>-0.960483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.776738</td>\n",
       "      <td>0.089624</td>\n",
       "      <td>-0.958261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.514772</td>\n",
       "      <td>0.092214</td>\n",
       "      <td>-0.960483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949783</td>\n",
       "      <td>0.083983</td>\n",
       "      <td>-0.947296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.926121</td>\n",
       "      <td>0.112477</td>\n",
       "      <td>-0.951696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.570990</td>\n",
       "      <td>0.064663</td>\n",
       "      <td>-0.953904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.884549</td>\n",
       "      <td>0.042766</td>\n",
       "      <td>-0.953904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.548302</td>\n",
       "      <td>0.184028</td>\n",
       "      <td>-0.960468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.910278</td>\n",
       "      <td>0.133006</td>\n",
       "      <td>-0.956082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.532501</td>\n",
       "      <td>0.091771</td>\n",
       "      <td>-0.962676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.801498</td>\n",
       "      <td>0.055863</td>\n",
       "      <td>-0.947310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500660</td>\n",
       "      <td>0.020560</td>\n",
       "      <td>-0.947310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.827647</td>\n",
       "      <td>0.178181</td>\n",
       "      <td>-0.964854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.822975</td>\n",
       "      <td>0.189077</td>\n",
       "      <td>-0.953889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.724234</td>\n",
       "      <td>0.173608</td>\n",
       "      <td>-0.962661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.749926</td>\n",
       "      <td>0.199376</td>\n",
       "      <td>-0.962661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.859428</td>\n",
       "      <td>0.127287</td>\n",
       "      <td>-0.945103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.644510</td>\n",
       "      <td>0.164056</td>\n",
       "      <td>-0.960468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.828976</td>\n",
       "      <td>0.019168</td>\n",
       "      <td>-0.936316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.743530</td>\n",
       "      <td>0.194228</td>\n",
       "      <td>-0.964854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.769741</td>\n",
       "      <td>0.199888</td>\n",
       "      <td>-0.958261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.992846</td>\n",
       "      <td>0.170078</td>\n",
       "      <td>-0.945088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.724126</td>\n",
       "      <td>0.182436</td>\n",
       "      <td>-0.958275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.815153</td>\n",
       "      <td>0.193372</td>\n",
       "      <td>-0.960454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.649822</td>\n",
       "      <td>0.152104</td>\n",
       "      <td>-0.962647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.854184</td>\n",
       "      <td>0.164077</td>\n",
       "      <td>-0.962661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.999474</td>\n",
       "      <td>0.137290</td>\n",
       "      <td>-0.958275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.776689</td>\n",
       "      <td>0.175773</td>\n",
       "      <td>-0.964854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.887956</td>\n",
       "      <td>0.156683</td>\n",
       "      <td>-0.962676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.705716</td>\n",
       "      <td>0.123661</td>\n",
       "      <td>-0.953889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.659912</td>\n",
       "      <td>0.113381</td>\n",
       "      <td>-0.962661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.750300</td>\n",
       "      <td>0.142905</td>\n",
       "      <td>-0.960468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.792356</td>\n",
       "      <td>0.193894</td>\n",
       "      <td>-0.967047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.629351</td>\n",
       "      <td>0.196983</td>\n",
       "      <td>-0.951682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.798146</td>\n",
       "      <td>0.151505</td>\n",
       "      <td>-0.962661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.683249</td>\n",
       "      <td>0.186968</td>\n",
       "      <td>-0.962661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.744353</td>\n",
       "      <td>0.166970</td>\n",
       "      <td>-0.964854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.617039</td>\n",
       "      <td>0.160850</td>\n",
       "      <td>-0.953904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.587755</td>\n",
       "      <td>0.070860</td>\n",
       "      <td>-0.956097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.711060</td>\n",
       "      <td>0.035463</td>\n",
       "      <td>-0.945103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_child_weight  colsample_bytree  learning_rate    losses\n",
       "0        19.0               2.0          0.585235       0.033688 -0.945117\n",
       "1         5.0               2.0          0.727186       0.105956 -0.962676\n",
       "2         6.0               2.0          0.959945       0.154804 -0.960454\n",
       "3         6.0               2.0          0.950012       0.120686 -0.960483\n",
       "4        16.0               2.0          0.674336       0.142392 -0.960454\n",
       "5         8.0               2.0          0.863774       0.106579 -0.960454\n",
       "6        14.0               2.0          0.957521       0.079111 -0.956082\n",
       "7        19.0               2.0          0.695018       0.095213 -0.960483\n",
       "8         9.0               2.0          0.684442       0.147520 -0.960483\n",
       "9         8.0               1.0          0.592116       0.081179 -0.953904\n",
       "10        6.0               2.0          0.614798       0.076255 -0.960483\n",
       "11        7.0               2.0          0.776738       0.089624 -0.958261\n",
       "12        8.0               2.0          0.514772       0.092214 -0.960483\n",
       "13       19.0               1.0          0.949783       0.083983 -0.947296\n",
       "14       10.0               1.0          0.926121       0.112477 -0.951696\n",
       "15        6.0               2.0          0.570990       0.064663 -0.953904\n",
       "16        7.0               2.0          0.884549       0.042766 -0.953904\n",
       "17       18.0               2.0          0.548302       0.184028 -0.960468\n",
       "18        6.0               2.0          0.910278       0.133006 -0.956082\n",
       "19        9.0               2.0          0.532501       0.091771 -0.962676\n",
       "20       11.0               1.0          0.801498       0.055863 -0.947310\n",
       "21       12.0               1.0          0.500660       0.020560 -0.947310\n",
       "22       14.0               2.0          0.827647       0.178181 -0.964854\n",
       "23       14.0               1.0          0.822975       0.189077 -0.953889\n",
       "24       16.0               2.0          0.724234       0.173608 -0.962661\n",
       "25       13.0               2.0          0.749926       0.199376 -0.962661\n",
       "26       16.0               1.0          0.859428       0.127287 -0.945103\n",
       "27       15.0               2.0          0.644510       0.164056 -0.960468\n",
       "28       12.0               2.0          0.828976       0.019168 -0.936316\n",
       "29       17.0               2.0          0.743530       0.194228 -0.964854\n",
       "30       18.0               2.0          0.769741       0.199888 -0.958261\n",
       "31       17.0               1.0          0.992846       0.170078 -0.945088\n",
       "32       20.0               2.0          0.724126       0.182436 -0.958275\n",
       "33       14.0               2.0          0.815153       0.193372 -0.960454\n",
       "34       17.0               2.0          0.649822       0.152104 -0.962647\n",
       "35       15.0               2.0          0.854184       0.164077 -0.962661\n",
       "36       13.0               2.0          0.999474       0.137290 -0.958275\n",
       "37       20.0               2.0          0.776689       0.175773 -0.964854\n",
       "38       11.0               2.0          0.887956       0.156683 -0.962676\n",
       "39       15.0               1.0          0.705716       0.123661 -0.953889\n",
       "40       18.0               2.0          0.659912       0.113381 -0.962661\n",
       "41       17.0               2.0          0.750300       0.142905 -0.960468\n",
       "42       16.0               2.0          0.792356       0.193894 -0.967047\n",
       "43       19.0               1.0          0.629351       0.196983 -0.951682\n",
       "44       19.0               2.0          0.798146       0.151505 -0.962661\n",
       "45       16.0               2.0          0.683249       0.186968 -0.962661\n",
       "46       18.0               2.0          0.744353       0.166970 -0.964854\n",
       "47       20.0               1.0          0.617039       0.160850 -0.953904\n",
       "48       15.0               2.0          0.587755       0.070860 -0.956097\n",
       "49       17.0               2.0          0.711060       0.035463 -0.945103"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = [loss_dict['loss'] for loss_dict in trial_val.results]\n",
    "result_df = pd.DataFrame({'max_depth': trial_val.vals['max_depth'],\n",
    "                          'min_child_weight': trial_val.vals['min_child_weight'],\n",
    "                          'colsample_bytree': trial_val.vals['colsample_bytree'],\n",
    "                          'learning_rate': trial_val.vals['learning_rate'],\n",
    "                          'losses': losses\n",
    "                         }\n",
    "                        )\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
